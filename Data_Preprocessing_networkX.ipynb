{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the biggest investment managers\n",
    "* scraping the website: https://www.advratings.com/top-asset-management-firms containing the list of the top asset mangement firms\n",
    "* processing the name of the of the company\n",
    "* storing the list of all the companies in funds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funds_list = []\n",
    "\n",
    "url = 'https://www.advratings.com/top-asset-management-firms'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "for row in soup.findAll('table')[0].tbody.findAll('tr'):\n",
    "    company = str(row.findAll('td')[1].contents)\n",
    "    company = re.split(r'<|>', company)\n",
    "    if(len(company) > 2):\n",
    "        #exluding any special chars and wite spaces from company names\n",
    "        company = ''.join(e for e in company[2] if e.isalnum())\n",
    "        funds_list.append(company.upper())\n",
    "    else:\n",
    "        company = re.split(r'([\\'|\\'])', company[0])\n",
    "        #exluding any special chars and wite spaces from company names\n",
    "        company = ''.join(e for e in company[2] if e.isalnum())\n",
    "        funds_list.append(company.upper())\n",
    "\n",
    "#deleting the first record (remainder of the header)\n",
    "funds_list = funds_list[1:]\n",
    "len(funds_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a dictionary of {Year : URL list} \n",
    "* Getting path to all 13F-HR filing per quarter \n",
    "* Each file corresponds to quater 1, the files are for years 2021 - 2018 \n",
    "* Choosing only files from **funds_list** - list of the top asset investment managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_years_urls = {}\n",
    "path = 'https://www.sec.gov/Archives/'\n",
    "companies = []\n",
    "\n",
    "file_2020_Q2 = open('/home/ivana/Environments/Data_Preprocessing/13F/2020Q2.txt', 'r')\n",
    "file_2020_Q3 = open('/home/ivana/Environments/Data_Preprocessing/13F/2020Q3.txt', 'r')\n",
    "file_2020_Q4 = open('/home/ivana/Environments/Data_Preprocessing/13F/2020Q4.txt', 'r')\n",
    "\n",
    "file_2021 = open('/home/ivana/Environments/Data_Preprocessing/13F/13F_2021.txt', 'r')\n",
    "file_2020 = open('/home/ivana/Environments/Data_Preprocessing/13F/13F_2020.txt', 'r')\n",
    "file_2019 = open('/home/ivana/Environments/Data_Preprocessing/13F/13F_2019.txt', 'r')\n",
    "file_2018 = open('/home/ivana/Environments/Data_Preprocessing/13F/13F_2018.txt', 'r')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "files = [file_2021, file_2020, file_2019, file_2018, file_2020_Q2, file_2020_Q3, file_2020_Q4]\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    forms_url = []\n",
    "    for line in file:\n",
    "        \n",
    "        #parsing out the company name from the list\n",
    "        company = re.findall(r'13F-HR\\s*\\d*([\\D+\\s\\D+]*)\\s*\\d*', line)\n",
    "        \n",
    "        #string processing to get uniform formatting\n",
    "        company = ''.join(e for e in company)\n",
    "        company = company.replace(' ', '')\n",
    "        company = re.sub('\\d', '', company)\n",
    "        company = company.upper()\n",
    "\n",
    "        \n",
    "        #finding the investment managers that match the list of the top investment mangers *fund_list*\n",
    "        for name in funds_list:\n",
    "            if (company in name or name in company) and len(company) > 3:\n",
    "                splitted = line.split()\n",
    "                forms_url.append(path + splitted[-1])\n",
    "                \n",
    "    #adding a key:value pair to a dict. - contains \n",
    "    all_years_urls[file.name.split('/')[-1]] = forms_url\n",
    "\n",
    "\n",
    "#finding out how many \n",
    "len(all_years_urls.get('2020Q4.txt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a nested dictionary {cik : { issuer : total_amount } }\n",
    "\n",
    "### Long running time no need to run -> the results per years are stored in **data20XX.json**\n",
    "\n",
    "* {cik1 : { issuer1 : total_amount, issuer2 : total_amount ...etc}, cik2 : {} ..etc }\n",
    "* for each investment manager *cik* we get a dictionary of all of the companies it invested into = *issuer*\n",
    "* for each issuer company *issuer* we get a value corresponding to the **total amount** of stocks\n",
    "* you can obtain data for a desired year (1st quarter of 2021 - 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list of all year (keys in all_years_urls)\n",
    "years = ['13F_2021.txt', '13F_2020.txt', '13F_2019.txt', '13F_2018.txt', '2020Q2.txt', '2020Q3.txt', '2020Q4.txt']\n",
    "\n",
    "data_dict = {} #nested dictionary for each CIK contains a dictionary of {issuer : total_amount}\n",
    "names_dict = {} #a dictionary linking cik to the name \n",
    "\n",
    "\n",
    "# getting the data frame for a given year \n",
    "for url in all_years_urls.get(years[1]): \n",
    "    page = requests.get(url)\n",
    "    data = page.text\n",
    "    soup = BeautifulSoup(data, \"lxml\")\n",
    "    \n",
    "    cik_pattern = r'\\s*CENTRAL INDEX KEY:\\s*(\\w[\\w*|\\s*]*)\\n'\n",
    "    cik_key = re.findall(cik_pattern, data)\n",
    "    cik_key = str(cik_key[0]) if cik_key != [] else None\n",
    "    data_dict[cik_key] = {}\n",
    "    \n",
    "    name_pattern = r'\\s*COMPANY CONFORMED NAME:\\s*(\\w[\\w*|\\s*]*)\\n*'\n",
    "    name = re.findall(name_pattern, data)\n",
    "    name = name[0].split('\\n') if name != [] else None \n",
    "    names_dict[cik_key] = name[0] if name != None else None\n",
    " \n",
    "    \n",
    "    stocklist = soup.find_all('infotable')\n",
    "\n",
    "    for s in stocklist:\n",
    "\n",
    "        if s.find(\"ns1:nameofissuer\") != None:\n",
    "            # Company name\n",
    "            n = s.find(\"ns1:nameofissuer\").string\n",
    "            if n in data_dict[cik_key].keys():\n",
    "                #Create only a record if the issuer is unique, oterwise sum the amount of stocks\n",
    "                data_dict[cik_key][n] = data_dict[cik_key].get(n) + int(s.find(\"ns1:shrsorprnamt\").find(\"ns1:sshprnamt\").string)# Company name\n",
    "            else:\n",
    "                data_dict[cik_key][n] = int(s.find(\"ns1:shrsorprnamt\").find(\"ns1:sshprnamt\").string)\n",
    "        \n",
    "        else:\n",
    "            n = s.find(\"nameofissuer\").string\n",
    "            if n in data_dict[cik_key].keys():\n",
    "                #Create only a record if the issuer is unique, oterwise sum the amount of stocks\n",
    "                data_dict[cik_key][n] = data_dict[cik_key].get(n) + int(s.find(\"shrsorprnamt\").find(\"sshprnamt\").string)\n",
    "            else:\n",
    "                data_dict[cik_key][n] = int(s.find(\"shrsorprnamt\").find(\"sshprnamt\").string)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing data in json format\n",
    "*No need to run if json files are present in the directory*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing ciks/issuers with empty dictionary\n",
    "data_dict = dict(filter(lambda sub: sub[1], data_dict.items()))\n",
    "names_dict = dict(filter(lambda sub: sub[1], names_dict.items()))\n",
    "\n",
    "with open('/home/ivana/Environments/Data_Preprocessing/json/data2020.json', 'w') as json_file:\n",
    "    json.dump(data_dict, json_file)\n",
    "\n",
    "with open('/home/ivana/Environments/Data_Preprocessing/json/names2020.json', 'w') as json_file:\n",
    "    json.dump(names_dict, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the json data into dict\n",
    "### Just copy this step to notebook where you do your analysis to retrieve a dict of data for desired year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ivana/Environments/Data_Preprocessing/json/data2021.json') as json_file:\n",
    "    data_dict = json.load(json_file)\n",
    "    \n",
    "with open('/home/ivana/Environments/Data_Preprocessing/json/names2020_Q4.json') as json_file:\n",
    "    names_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Graphs with NetworkX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(list(data_dict.keys()), bipartite = 0)\n",
    "for cik in data_dict.keys():\n",
    "    issuers_dict = data_dict.get(cik)\n",
    "    G.add_nodes_from(list(issuers_dict.keys()), bipartite = 1)\n",
    "    edges = []\n",
    "    for issuer in issuers_dict:\n",
    "        tuple = (cik, issuer, {'amount' : issuers_dict.get(issuer)})\n",
    "        edges.append(tuple)\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization with PyVis\n",
    "## Making a smaller sample of data for better visualization\n",
    "* considering only issuers that more than once investment manager invested in\n",
    "* once the two investment manger are connected - only one of their investmet is dispayed\n",
    "* RED NODES = issuers\n",
    "* BLUE NODES = CIKs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making a smaller graph\n",
    "\n",
    "def visulailze_network(data_dict):\n",
    "    G_small = nx.Graph()\n",
    "    nt = Network()\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    #G_small.add_nodes_from(list(data_dict.keys()), bipartite = 0)\n",
    "    for cik1 in data_dict.keys():\n",
    "        issuers_dict1 = data_dict.get(cik1)\n",
    "        issuers_set1 = set(issuers_dict1.keys())\n",
    "\n",
    "        for cik2 in data_dict.keys():\n",
    "            issuers_dict2 = data_dict.get(cik2)\n",
    "            issuers_set2 = set(issuers_dict2.keys())\n",
    "            common = issuers_set1.intersection(issuers_set2)\n",
    "\n",
    "            if(bool(common) and cik1 != cik2):\n",
    "                nt.add_node(cik1, color = \"blue\")\n",
    "                nt.add_node(cik2, color = \"blue\")\n",
    "                for issuer in list(common)[:1]:\n",
    "\n",
    "                    nt.add_node(issuer, color = \"red\")\n",
    "                    nt.add_edge(cik1, issuer)\n",
    "                    nt.add_edge(cik2, issuer)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sizes(data_dict):\n",
    "    sizes = {}\n",
    "    for cik1 in data_dict.keys():\n",
    "        issuers_dict1 = data_dict.get(cik1)\n",
    "        issuers_set1 = set(issuers_dict1.keys())\n",
    "    \n",
    "        for cik2 in data_dict.keys():\n",
    "            issuers_dict2 = data_dict.get(cik2)\n",
    "            issuers_set2 = set(issuers_dict2.keys())\n",
    "            common = issuers_set1.intersection(issuers_set2)\n",
    "\n",
    "            if(bool(common) and cik1 != cik2):\n",
    "                \n",
    "                if cik1 in sizes.keys():\n",
    "                    sizes[cik1] += 1\n",
    "                else: \n",
    "                    sizes[cik1] = 1              \n",
    "                \n",
    "    return sizes\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network(data_dict):\n",
    "    \n",
    "    G_small = nx.Graph()\n",
    "    nt = Network()\n",
    "    count = 0\n",
    "    \n",
    "    sizes = get_sizes(data_dict)\n",
    "    for cik1 in sizes:\n",
    "        issuers_dict1 = data_dict.get(cik1)\n",
    "        issuers_set1 = set(issuers_dict1.keys())\n",
    "        for cik2 in sizes:\n",
    "            issuers_dict2 = data_dict.get(cik2)\n",
    "            issuers_set2 = set(issuers_dict2.keys())\n",
    "            common = issuers_set1.intersection(issuers_set2)\n",
    "            \n",
    "            if(bool(common) and cik1 != cik2):\n",
    "                nt.add_node(cik1, color = \"blue\", size = sizes.get(cik1))\n",
    "                nt.add_node(cik2, color = \"blue\", size = sizes.get(cik2))\n",
    "                nt.add_edge(cik1, cik2)\n",
    "    return nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = visualize_network(data_dict)\n",
    "\n",
    "# Visualization of the grapj\n",
    "nt.show_buttons(filter_=['physics'])\n",
    "nt.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2c9f8128c549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Visualization of the grapj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_buttons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'physics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nx.html'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nt' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
